# üß† LLM Providers

Multi-Agent Generator is **provider-agnostic**, thanks to [LiteLLM](https://docs.litellm.ai/).
You can seamlessly switch between **OpenAI**, **WatsonX**, **Ollama**, **Groq**, **Anthropic**, and **AIML API** ‚Äî using a single CLI flag or environment variable.

---

## üåç Supported Providers

| **Provider**    | **Default Model**             | **Best For**              | **Key Strengths**                                  |
| --------------- | ----------------------------- | ------------------------- | -------------------------------------------------- |
| **OpenAI**      | `gpt-4o-mini`                 | General-purpose reasoning | Best reasoning, reliability, context length        |
| **IBM WatsonX** | `llama-3-70b-instruct`        | Enterprise use            | Compliance, data governance, IBM Cloud integration |
| **Groq**        | `llama-3-8b`                  | Real-time pipelines       | Lightning-fast inference, low latency              |
| **Ollama**      | `llama3`, `mistral`           | Local development         | Privacy-friendly, runs fully offline               |
| **AIML API**    | `custom-finetuned`, `llama-2` | Flexible deployments      | Open, customizable, on-prem/cloud/hybrid           |
| **Anthropic**   | `claude-3-opus`               | Natural reasoning         | Long-context understanding, alignment focus        |

---

## ‚öôÔ∏è Environment Setup

Set up your API credentials in your environment variables.
Depending on your provider, you can configure one or more of the following:

### üîπ OpenAI

```bash
export OPENAI_API_KEY="your_openai_api_key"
```

### üîπ IBM WatsonX

```bash
export WATSONX_API_KEY="your_watsonx_api_key"
export WATSONX_PROJECT_ID="your_project_id"
export WATSONX_URL="your_instance_url"
```

### üîπ Ollama (local)

```bash
export OLLAMA_URL="http://localhost:11434"
```

### üîπ Groq

```bash
export GROQ_API_KEY="your_groq_api_key"
```

### üîπ AIML API

```bash
export AIML_API_KEY="your_aiml_api_key"
export AIML_BASE_URL="https://api.aimlapi.com/v1"
```

### üîπ Anthropic

```bash
export ANTHROPIC_API_KEY="your_anthropic_api_key"
```

---

## ‚ö° CLI Usage

You can switch providers instantly from the command line.

### OpenAI (default)

```bash
multi-agent-generator "Create a summarizer and QA assistant" --framework crewai
```

### WatsonX

```bash
multi-agent-generator "Build a document classifier" --framework langgraph --provider watsonx
```

### Groq

```bash
multi-agent-generator "Generate a real-time customer feedback analyzer" --framework crewai --provider groq
```

### Ollama (local)

```bash
multi-agent-generator "Build a ReAct chatbot for support" --framework react-lcel --provider ollama
```

### AIML API

```bash
multi-agent-generator "Create a custom marketing team workflow" --framework agno --provider aimlapi
```

### Anthropic

```bash
multi-agent-generator "Develop a debate-style reasoning agent" --framework react --provider anthropic
```

---

## üîÑ Switching Providers in Code

You can also use the provider argument programmatically if you integrate this library:

```python
from multi_agent_generator import generate_agents

agents = generate_agents(
    prompt="I need a content creation team",
    framework="crewai",
    provider="groq"   # or openai, watsonx, ollama, aimlapi, anthropic
)
```

---

## üß© Comparison Summary

| **Feature**               | **OpenAI** | **WatsonX**         | **Groq**      | **Ollama** | **AIML API**             | **Anthropic** |
| ------------------------- | ---------- | ------------------- | ------------- | ---------- | ------------------------ | ------------- |
| **Speed**                 | ‚ö°‚ö°         | ‚ö°                   | ‚ö°‚ö°‚ö°           | ‚ö°          | ‚ö°‚ö°                       | ‚ö°             |
| **Offline Support**       | ‚ùå          | ‚ùå                   | ‚öôÔ∏è Limited    | ‚úÖ          | ‚úÖ                        | ‚ùå             |
| **Latency**               | Medium     | Medium              | üî• Ultra-low  | Low        | Medium                   | Medium        |
| **Deployment**            | Cloud      | IBM Cloud           | Edge / Cloud  | Local      | Cloud / On-prem / Hybrid | Cloud         |
| **Customization**         | Low        | Medium              | Medium        | High       | üî• Very High             | Low           |
| **Security / Compliance** | High       | üî• Enterprise-grade | Medium        | Local      | High                     | High          |
| **Ease of Integration**   | ‚úÖ SDKs     | ‚úÖ Enterprise APIs   | ‚úÖ Simple REST | ‚úÖ Simple   | ‚úÖ REST/GraphQL           | ‚úÖ SDKs        |

---

## üí° Notes

* All providers are managed under **LiteLLM**, so configuration and switching remain consistent.
* You can override base URLs using `LITELLM_API_BASE` or provider-specific variables.
* Some frameworks (like **Agno**) currently support only OpenAI-compatible APIs ‚Äî support for more is coming soon.

---

## ‚ù§Ô∏è Contribute

If you‚Äôve tested or configured new models (like **Gemini**, **Cohere**, or **Mistral API**) with Multi-Agent Generator, feel free to open a PR to add them here.

